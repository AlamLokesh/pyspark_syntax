Loading and Saving Data:
    priority: 1
    description: Loading data into DataFrames from various formats, and saving it out again.

DataFrame Operations:
    priority: 2
    description: Adding, removing and modifying DataFrame columns.

Transforming Data:
    priority: 3
    description: Data conversions and other modifications.

Sorting and Searching:
    priority: 4
    description: Filtering, sorting, removing duplicates and more.

Grouping:
    priority: 5
    description: Group DataFrame data by key to perform aggregats like sums, averages, etc.

Joining DataFrames:
    priority: 6
    description: Joining and stacking DataFrames.

File Processing:
    priority: 7
    description: Loading File Metadata and Processing Files

Handling Missing Data:
    priority: 8
    description: Dealing with NULLs and NaNs in DataFrames.

Pandas:
    priority: 9
    description: Using Python's Pandas library to augment Spark. Some operations require the pyarrow library.

Data Profiling:
    priority: 10
    description: Extracting key statistics out of a body of data.

Time Series:
    priority: 11
    description: Techniques for dealing with time series data.

Performance:
    priority: 12
    description: A few performance tips and tricks.

Preamble:
    description: >
        PySpark Cheat Sheet

        ===================

        This cheat sheet will help you learn PySpark and write PySpark apps faster. Everything in here is fully functional PySpark code you can run or adapt to your programs.

        
        These snippets are licensed under the CC0 1.0 Universal License. That means you can freely copy and adapt these code snippets and you don't need to give attribution or include any notices.


        These snippets use DataFrames loaded from various data sources:

        - "Auto MPG Data Set" available from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/auto+mpg).

        - customer_spend.csv, a generated time series dataset.

        - date_examples.csv, a generated dataset with various date and time formats.


        These snippets were tested against the Spark 2.4.4 API. This page was last updated {last_updated}.


        Make note of these helpful links:

        - [Built-in Spark SQL Functions](https://spark.apache.org/docs/latest/api/sql/index.html)

        - [PySpark SQL Functions Source](https://spark.apache.org/docs/latest/api/python/_modules/pyspark/sql/functions.html)

        - [PySpark DataFrame Operations](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame)


        If you find this guide helpful and want an easy way to run Spark, check out [Oracle Cloud Infrastructure Data Flow](https://www.oracle.com/big-data/data-flow/), a fully-managed Spark service that lets you run Spark jobs at any scale with no administrative overhead. You can try Data Flow free.

Postscript:
    description:
