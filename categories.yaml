Loading and Saving Data:
    priority: 1
    description: Loading data into DataFrames from various formats, and saving it out again.

DataFrame Operations:
    priority: 2
    description: Adding, removing and modifying DataFrame columns.

Transforming Data:
    priority: 3
    description: Data conversions and other modifications.

Sorting and Searching:
    priority: 4
    description: Filtering, sorting, removing duplicates and more.

Grouping:
    priority: 5
    description: Group DataFrame data by key to perform aggregats like sums, averages, etc.

Joining DataFrames:
    priority: 6
    description: Joining and stacking DataFrames.

Handling Missing Data:
    priority: 7
    description: Dealing with NULLs and NaNs in DataFrames.

Pandas:
    priority: 8
    description: Using Python's Pandas library to augment Spark. Some operations require the pyarrow library.

Performance:
    priority: 10
    description: A few performance tips and tricks.

Data Profiling:
    priority: 9
    description: Extracting key statistics out of a body of data.

Preamble:
    description: >
        PySpark Cheat Sheet

        ===================

        This cheat sheet will help you learn PySpark and write PySpark apps faster. Everything in here is fully functional PySpark code you can run or adapt to your programs.

        
        These snippets are licensed under the CC0 1.0 Universal License. That means you can freely copy and adapt these code snippets and you don't need to give attribution or include any notices.


        The snippets below refer to DataFrames loaded from the "Auto MPG Data Set" available from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/auto+mpg). You can download that dataset or clone this repository to test the code yourself.

        
        These snippets were tested against the Spark 2.4.4 API. This page was last updated {last_updated}.


        Make note of these helpful links:

        - [Built-in Spark SQL Functions](https://spark.apache.org/docs/latest/api/sql/index.html)

        - [PySpark SQL Functions Source](https://spark.apache.org/docs/latest/api/python/_modules/pyspark/sql/functions.html)


        If you find this guide helpful and want an easy way to run Spark, check out [Oracle Cloud Infrastructure Data Flow](https://www.oracle.com/big-data/data-flow/), a fully-managed Spark service that lets you run Spark jobs at any scale with no administrative overhead. You can try Data Flow free.

Postscript:
    description:
